# Краулер новостных статей

## Описание проекта
Система предназначена для сбора новостных статей. Сбор может быть осуществлен в двух режимах. 
В первом режиме система с заданной периодичностью заходит на сайты-источники, указанные в конфиг-файлах и скачивает новые статьи. 
Во втором режиме система использует сервис Яндекс.Новостей для получения ссылок на старые новости, после чего проходит по этим ссылкам и получает тексты статей.

Система состоит из нескольких основных компонентов:
* менеджер, контролирующий сбор данных системой;
* работники, осуществляющие непосредственно загрузку данных из сети с сайтов источников (Лента, РБК и т. д.);
* сервер базы данных;
* сервер системы обмена сообщениями.

Так как система разрабатывалась распределенной, то все компоненты могут быть запущены как на одном компьютере, так и на нескольких.


## Требования
Для работы краулера понадобится графовая база данных Neo4j, менеджер очередей celery, rabbitmq и tor (для сбора архивных новостей через Яндекс.Новости), а также некоторые другие компоненты.

##### База данных Neo4j
[Docker-контейнер Neo4j](https://hub.docker.com/_/neo4j/ "Docker-контейнер Neo4j"). Чтобы после завершения работы контейнера не потерять данные, сохраненные в базе, при запуске контейнера следует прилинковать к нему какую-либо директорию на хост-машине.

##### Система обмена сообщениями RabbitMQ
[Docker-контейнер RabbitMQ](https://hub.docker.com/_/rabbitmq/ "Docker-контейнер RabbitMQ"). Для системы обмена сообщениями в принципе нет необходимости сохранять состояние очереди после перезапуска контейнера, т. к. как правило сбор данных осуществляется в пределах одной рабочей сессии. А сами сообщения не должные накапливаться. Однако это может произойти при использовании опций PROCESS_RECURSIVE_LINKS и PROCESS_RELATED_LINKS, что приведет к экспоненциальному росту поличества заданий в очереди вплоть до полного исчерпания памяти и блокированию работы системы. Поэтому использовать данные опции не рекомендуется.

##### Прочие зависимости
Прочие зависимости, необходимые для работы краулера собраны в файле `requirements.txt`.
Для установки зависимостей выполнить команду `pip install -r requirements.txt`. Рекомендуется для этого создать отдельное виртуальное окружение (`virtualenv`), либо использовать docker-контейнер.


## Файлы проекта
##### Скрипты, с коротыми взаимодействует пользователь:
* `archive_manager.py` - скрипт, запускающий менеджер сбора архивных новостей
* `real_time_manager.py` - скрипт, запускающий менеджер сбора новостей в режиме реального времени
* `newscollector_worker.py` - скрипт, который необходимо запускать на компьютерах-работниках
* `settings.py` - настройки программы
* `local_settings.py` - настройки программы для конкретной машины, файл не включен в репозиторий
* `sources_config.py` - конфиг-файл для сайтов-источников
* `sources_config_example.py` - описание конфиг-файла для сайтов-источников
* `sources_dtp_config.py` - конфиг-файл для сайтов-источников по тематике ДТП
* `tests/test_config.py` - тесты для конфиг файлов
* `celeryconfig.py` - сейчас не используется

##### Скрипты, использующиеся программой:
* `db_functions.py` - функции для работы с Neo4j
* `generic_functions.py` - различные функции общего назначения
* `models.py` - описание моделей данных
* `my_exceptions.py` - классы вызываемых исключений
* `yandex_news.py` - класс, описывающий взаимодействие с Яндекс.Новостями


## TODO list:
* интерфейс командной строки для запуска
